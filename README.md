# DISTRIBUTED-DATA-PROCESSING

*COMPANY* : CODTECH IT SOLUTIONS

*NAME* : Mohammed Faraz

*INTERN  ID* : CT04WE60

*DOMAIN* : BIG DATA

*DURATION* : 4 WEEKS

*MENTOR* : NEELA SANTOSH

*DESCRIPTION*

This project analyzes large datasets using Apache Spark, demonstrating filtering, grouping, and aggregations. It includes a Python script, analyze_data.py, that processes data, saving results to Parquet. Follow the README.md for setup, data placement, and execution.


*DEAILS* :

Clear Project Structure: The repository is organized with clear directories for source code, data, and output.

Comprehensive README.md: The README.md file provides detailed instructions on how to use the project, including installation, usage, and examples.

requirements.txt: This file lists the project's dependencies, making it easy for others to set up the environment.

Error Handling: The script includes error handling to catch potential issues during Spark operations.

Command-Line Arguments: The script now accepts the dataset file path as a command-line argument, making it more flexible.

Output Path: The results are saved to a specific output path.

Virtual Environment Instructions: The README includes instructions on how to create and activate a virtual environment.

Licensing: added a licensing section.

Sys arguments: used sys.argv to get the input file path.


*OUTPUT* :  output/results.parquet (Output):

This directory will contain the output of the Spark analysis.
